{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# First Assignment of Data Mining: Classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:43.016268Z",
     "end_time": "2023-04-16T14:51:43.061726Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compacting the files\n",
    "The first task is converting the excel data into parquet files for faster processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.write import write_parquet\n",
    "from src.read import read_xlsx\n",
    "\n",
    "path = 'data/'\n",
    "\n",
    "# for the existing customers\n",
    "fname = 'existing-customers.xlsx'\n",
    "df = read_xlsx(path + fname)\n",
    "new_fname = fname.split('.')[0] + '.parquet'\n",
    "write_parquet(df, path + new_fname)\n",
    "\n",
    "# for the potential customers\n",
    "fname = 'potential-customers.xlsx'\n",
    "df = read_xlsx(path + fname)\n",
    "new_fname = fname.split('.')[0] + '.parquet'\n",
    "write_parquet(df, path + new_fname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:43.021663Z",
     "end_time": "2023-04-16T14:51:48.436596Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning and Preprocessing\n",
    "We are interested in the distributions obtained from the imputation of the data\n",
    "so we will plot the distributions of the imputed columns before and after\n",
    "imputation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from src.read import read_parquet\n",
    "from src.impute import impute_decision_tree\n",
    "from src.clean import remove_rowid\n",
    "\n",
    "path = 'data/'\n",
    "fname = 'existing-customers.parquet'\n",
    "\n",
    "# read the dataset\n",
    "df = read_parquet(path + fname)\n",
    "\n",
    "# remove the rowid column\n",
    "df = remove_rowid(df)\n",
    "\n",
    "# get the null columns\n",
    "null_columns = df.columns[df.isnull().any()]\n",
    "\n",
    "# plot the distributions before the imputation\n",
    "for null_col in null_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[null_col].sort_values(), bins=len(df[null_col].unique()),\n",
    "                 stat='count', discrete=True)\n",
    "    plt.title(null_col)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# get the indices of the imputed columns\n",
    "indices_dict = dict()\n",
    "for null_col in null_columns:\n",
    "    indices_dict[null_col] = df[df[null_col].isnull()].index\n",
    "\n",
    "# impute the missing values\n",
    "df = impute_decision_tree(df)\n",
    "\n",
    "# plot the distributions after the imputation\n",
    "for null_col in null_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df.iloc[indices_dict[null_col]][null_col].sort_values(),\n",
    "                 bins=len(df[null_col].unique()), stat='count', discrete=True)\n",
    "    plt.title(null_col + ' (imputed)')\n",
    "    plt.xticks(rotation=45, ha='center')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:48.438928Z",
     "end_time": "2023-04-16T14:51:49.926239Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now do the rest of the cleaning and preprocessing to make it ready\n",
    "for the classifiers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.clean import change_class\n",
    "from src.encode import dummify\n",
    "\n",
    "# change the class column to a binary column\n",
    "df = change_class(df)\n",
    "\n",
    "# dummify the dataframe\n",
    "df = dummify(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:49.932042Z",
     "end_time": "2023-04-16T14:51:49.984598Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset splits\n",
    "Our next task is to split the data into training and testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.dataset_split import get_stratified_kfold_split, get_stratified_split\n",
    "\n",
    "# create a train and test set\n",
    "true_train_X, true_test_X, true_train_y, true_test_y = get_stratified_split(df)\n",
    "df = pd.concat([true_train_X, true_train_y], axis=1)\n",
    "split = list(get_stratified_kfold_split(df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:49.953392Z",
     "end_time": "2023-04-16T14:51:49.984762Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the Classifier Results\n",
    "import all the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.models.gradient_boosted_trees_classifier import gradient_boosted_trees\n",
    "from src.models.decision_tree_classifier import decision_tree\n",
    "from src.models.random_forest_classifier import random_forest\n",
    "from src.models.knn_classifier import knn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:49.966797Z",
     "end_time": "2023-04-16T14:51:49.984828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate all the models and store all the results in a dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate\n",
    "from src.dataset_split import get_X_y\n",
    "\n",
    "costs_dict = {'decision tree': list(),\n",
    "              'gradient boosted trees': list(),\n",
    "              'knn classifier': list(),\n",
    "              'random forest': list()}\n",
    "scores_df = pd.DataFrame(\n",
    "    {'model': list(), 'accuracy': list(), 'precision': list(),\n",
    "     'recall': list(), 'f1': list(), 'cost': list(),\n",
    "     'fold': list()})\n",
    "for fold, (train_idces, test_idces) in enumerate(split):\n",
    "    train_df = df.iloc[train_idces]\n",
    "    test_df = df.iloc[test_idces]\n",
    "    train_X, train_y = get_X_y(train_df)\n",
    "    test_X, test_y = get_X_y(test_df)\n",
    "\n",
    "    model = decision_tree(train_X, train_y)\n",
    "    score, precision, recall, f1, cost = evaluate(model, test_X, test_y,\n",
    "                                                  verbose=False)\n",
    "    scores_df = pd.concat([scores_df, pd.DataFrame(\n",
    "        {'model': ['decision tree'], 'accuracy': [score],\n",
    "         'precision': [precision], 'recall': [recall], 'f1': [f1],\n",
    "         'cost': [cost],\n",
    "         'fold': [fold]})])\n",
    "    costs_dict['decision tree'].append(cost)\n",
    "\n",
    "    model = gradient_boosted_trees(train_X, train_y)\n",
    "    score, precision, recall, f1, cost = evaluate(model, test_X, test_y,\n",
    "                                                  verbose=False)\n",
    "    scores_df = pd.concat([scores_df, pd.DataFrame(\n",
    "        {'model': ['gradient boosted trees'], 'accuracy': [score],\n",
    "         'precision': [precision], 'recall': [recall], 'f1': [f1],\n",
    "         'cost': [cost],\n",
    "         'fold': [fold]})])\n",
    "    costs_dict['gradient boosted trees'].append(cost)\n",
    "\n",
    "    model = knn(train_X, train_y, 5)\n",
    "    score, precision, recall, f1, cost = evaluate(model, test_X, test_y,\n",
    "                                                  verbose=False)\n",
    "    scores_df = pd.concat([scores_df, pd.DataFrame(\n",
    "        {'model': ['knn classifier'], 'accuracy': [score],\n",
    "         'precision': [precision], 'recall': [recall], 'f1': [f1],\n",
    "         'cost': [cost],\n",
    "         'fold': [fold]})])\n",
    "    costs_dict['knn classifier'].append(cost)\n",
    "\n",
    "    model = random_forest(train_X, train_y)\n",
    "    score, precision, recall, f1, cost = evaluate(model, test_X, test_y,\n",
    "                                                  verbose=False)\n",
    "    scores_df = pd.concat([scores_df, pd.DataFrame(\n",
    "        {'model': ['random forest'], 'accuracy': [score],\n",
    "         'precision': [precision], 'recall': [recall], 'f1': [f1],\n",
    "         'cost': [cost], 'fold': [fold]})])\n",
    "    costs_dict['random forest'].append(cost)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:51:49.972280Z",
     "end_time": "2023-04-16T14:52:16.340547Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the dataframe to a parquet file for later analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df.to_parquet(path + 'scores.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:52:16.340676Z",
     "end_time": "2023-04-16T14:52:16.340832Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print out the average costs of the models and make a violin plot out of the\n",
    "results of the different folds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model, costs in costs_dict.items():\n",
    "    print(model)\n",
    "    print(np.average(costs))\n",
    "    print()\n",
    "    # make a violin plot out of the costs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:52:16.340902Z",
     "end_time": "2023-04-16T14:52:16.342082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a violin plot out of the costs of the different models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='model', y='cost', data=scores_df)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:52:16.341128Z",
     "end_time": "2023-04-16T14:52:16.428078Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning\n",
    "We can now clearly see that the lowest cost comes from the gradient boosted\n",
    "trees classifier. Therefore, we will use this model and tune hyperparameters\n",
    "on this model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.optimization import simulated_annealing, get_model_from_state\n",
    "\n",
    "state = simulated_annealing(initial_state=(.4, 15, 20, .8, .2), split=split,\n",
    "                            df=df)\n",
    "model = get_model_from_state(state)\n",
    "model.fit(true_train_X, true_train_y)\n",
    "evaluate(model, true_test_X, true_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:52:16.430020Z",
     "end_time": "2023-04-16T15:05:20.239550Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Results\n",
    "Obtain the final results by training the calculated model with the state on the\n",
    "whole labeled dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model_from_state(state)\n",
    "model.fit(df.drop(columns='class'), df['class'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:05:20.240400Z",
     "end_time": "2023-04-16T15:05:20.391102Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the potential customers and make predictions on them, we also need to\n",
    "impute the values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "potential_customers_fname = 'potential-customers.parquet'\n",
    "\n",
    "# load the dataset\n",
    "potential_customers_df = read_parquet(path + potential_customers_fname)\n",
    "\n",
    "# keep the id column somewhere else\n",
    "potential_customers = potential_customers_df['RowID']\n",
    "\n",
    "# remove the rowid column\n",
    "potential_customers_df = remove_rowid(potential_customers_df)\n",
    "\n",
    "# impute the missing values\n",
    "potential_customers_df = impute_decision_tree(potential_customers_df)\n",
    "\n",
    "# dummify the dataframe\n",
    "potential_customers_df = dummify(potential_customers_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:05:20.390143Z",
     "end_time": "2023-04-16T15:05:20.654716Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform prediction on the obtained dataframe `potential_customers_df`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_set = set(df.columns)\n",
    "potential_customers_set = set(potential_customers_df.columns)\n",
    "df_set.difference(potential_customers_set)\n",
    "for col in df_set.difference(potential_customers_set):\n",
    "    if col == 'class':\n",
    "        continue\n",
    "    potential_customers_df[col] = 0\n",
    "predictions = model.predict(potential_customers_df)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:05:20.655414Z",
     "end_time": "2023-04-16T15:05:20.735281Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the costs of the predictions. We take a rate of\n",
    "0.63 = 1385/(824+1385) = TP/(FP+TP) as the ratio to work with, also known as the\n",
    "precision. This means that approximately 63% of the positively predicted\n",
    "customers can win us money."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the total positively marked customers\n",
    "total = sum(predictions)\n",
    "\n",
    "# calculate the # of true positives\n",
    "TP = 1385\n",
    "FP = 824\n",
    "ratio = TP / (TP + FP)\n",
    "\n",
    "true_positives = int(total * ratio)\n",
    "false_positives = total - true_positives\n",
    "\n",
    "# calculate the costs\n",
    "cost = (0.05 * 310 * false_positives + 10) - (0.1 * 980 * true_positives + 10)\n",
    "cost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:05:20.737597Z",
     "end_time": "2023-04-16T15:05:20.739268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We approximately are going to gain around 300.000 euros with this model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write the row ids of the customers to send the promotion to in a txt file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write the row ids to a txt file\n",
    "with open('data/row_ids.txt', 'w') as f:\n",
    "    for row_id, prediction in enumerate(predictions):\n",
    "        # skip if the prediction is 0\n",
    "        if not prediction:\n",
    "            continue\n",
    "        f.write(str(row_id) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T15:05:20.741481Z",
     "end_time": "2023-04-16T15:05:20.744764Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
